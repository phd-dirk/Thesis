\documentclass[../../index.tex]{subfiles}

\begin{document}
  \chapter{Derivation of the used inverse covariance matrix from the Aleph
    data}
  While performing a \textbf{Generalized least squares} (GLS) we estimate our
  regression coefficients $\hat\beta$ as follows:
  \begin{equation}
    \hat\beta = \underset{b}{\operatorname {argmin}}(\mathbf{y} - \mathbf{X} \mathbf{b}
    )^{\mathtt{T}} \mathbf{\Omega}^{-1} (\mathbf{y} - \mathbf{X} \mathbf{b}),
  \end{equation}
  with $\mathbf{b}$ being an candidate estimate of $\beta$, $\mathbf{X}$ being the
  design matrix, $\mathbf{y}$ being the response values and
  $\mathbf{\Omega}^{-1}$ being the \textbf{inverse covariance matrix}.

  The Aleph data includes the \textbf{standard error} (SE), which
  are equal to the \textbf{standard deviation} as per definition. Furthermore
  Aleph provides the \textbf{correlation coefficients} of the errors. We will
  use these two quantities in combination with \textbf{Gaussian error
    propagation} to derive derive an approximation of the covariance matrix.

  \section{Propagation of experimental errors and correlation}
  Let $\{f_k(x_1, x_2, \cdots x_n)\}$ be a set of $m$ functions, which a linear
  combinations of n variables $x_1, x_2, \cdots x_n$ with combination
  coefficients $A_{k1}, A_{k2}, \cdots A_{kn}$, where $k \in \{1, 2, \cdots,
  m\}$. Let the covariance matrix of $x_n$ be denoted by
  \begin{equation}
    \Sigma^x =
    \begin{pmatrix}
      \sigma_{1}^2 & \sigma_{12}  & \sigma_{13}  & \cdots \\
      \sigma_{12}  & \sigma_{2}^2 & \sigma_{23}  & \cdots \\
      \sigma_{13}  & \sigma_{23}  & \sigma_{3}^2 & \cdots \\
      \vdots      & \vdots       & \vdots      &  \ddots
    \end{pmatrix}.
  \end{equation}
  Then the covariance matrix of the functions $\Sigma^f$ is given by
  \begin{equation}
    \Sigma_{ij}^f = \sum_k^n \sum_l^n A_{ik} \sum_{kl}^x A_{jl}, \quad \Sigma^f= A \Sigma^x A^{\mathtt{T}}.
  \end{equation}

  In our case we are dealing with non-linear functions, which we will
  linearized with the help of the \textbf{Taylor expansion}
  \begin{equation}
    f_k \approx f_k^0 + \sum_i^n \frac{\partial f_k}{\partial x_i} x_i, \quad f \approx f^0 + Jx.
  \end{equation}
  Therefore, the propagation of error follows from the linear case, replacing
  the Jacobian matrix with the combination coefficients ($J = A$)
  % \begin{equation}
  %   \Sigma^f = J \Sigma^x J^{\mathtt{T}}.

  \chapter{Coefficients}
  \label{app:coefficients}
  \section{$\beta$ function}
  \label{sec:betaCoefficients}
  There are several conventions for defining the $\beta$ coefficients, depending
  on a minus sign and/or a factor of two (if one substitues $\mu \to \mu^2$) in
  the $\beta$-function \ref{eq:betaFunction}. We follow the convention from
  Pascual and Tarrach (except for the minus sign) and have taken the values from
  \cite{Boito2011}
  \begin{align}
    \beta_1 &= \frac{1}{6} (11N_c - 2N_f), \\
    \beta_2 &= \frac{1}{12} (17N_c^2 - 5N_cN_f - 3 C_fN_f), \\
    \beta_3 &= \frac{1}{32}\left( \frac{2857}{54}N_c^3 -\frac{1415}{54}N_c^2 N_f + \frac{79}{54} N_c N_f^2 - \frac{205}{18} N_c C_f N_f + \frac{11}{9} C_f N_f^2 + C_f^2 N_f \right), \\
    \beta_4 &= \frac{140599}{2304} + \frac{445}{16}\zeta_3,
  \end{align}
  where we used $N_f=3$ and $N_c=3$ for $\beta_4$.

  \section{Anomalous mass dimension}
  \label{app:gammaCoefficients}
  \begin{align}
    \gamma_1 &= \frac{3}{2}C_f, \\
    \gamma_2 &= \frac{C_f}{48}(97 N_c + 9 C_f - 10N_f), \\
    \gamma_3 &= \frac{C_f}{32}\left[ \frac{11413}{108} N_c^2 - \frac{129}{4} N_cC_f - \left( \frac{278}{27} + 24 \zeta_3 \right) N_c N_f + \frac{129}{2} C_f^2 - (23 - 24 \zeta_3) C_f N_f - \frac{35}{27} N_f^2 \right], \\
    \gamma_4 &= \frac{2977517}{20736} - \frac{9295}{216}\zeta_3 + \frac{135}{8}\zeta_4 - \frac{125}{6}\zeta_5,
  \end{align}
  where $N_c$ is the number of colours, $N_f$ the number of flavours and
  $C_f=(N_c^2-1)/2N_c$. We fixed furthermore fixed
  $N_f=3$ and $N_c=3$ for $\gamma_4$.


  \section{Adler function}

  \end{document}